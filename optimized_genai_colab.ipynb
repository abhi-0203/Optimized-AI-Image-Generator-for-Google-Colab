{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQJWO7AmnRqlItpsjL/gdI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi-0203/Optimized-AI-Image-Generator-for-Google-Colab/blob/main/optimized_genai_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip install diffusers==0.32.2\n",
        "!pip install transformers==4.49"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IwO6lySmzRlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from diffusers import DiffusionPipeline, AutoencoderKL\n",
        "import torch\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "from PIL import Image\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output in Colab\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "class OptimizedImageGenerator:\n",
        "    def __init__(self):\n",
        "        self.pipe = None\n",
        "        self.current_model = None\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.generator = None\n",
        "\n",
        "        # Setup for Colab optimization\n",
        "        if torch.cuda.is_available():\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    def check_memory(self):\n",
        "        \"\"\"Check and display current GPU memory usage\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
        "            reserved = torch.cuda.memory_reserved() / 1024**3    # GB\n",
        "            return f\"üîç GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\"\n",
        "        return \"CPU mode - No GPU memory tracking\"\n",
        "\n",
        "    def clear_memory(self):\n",
        "        \"\"\"Aggressive memory cleanup for Colab\"\"\"\n",
        "        if hasattr(self, 'pipe') and self.pipe is not None:\n",
        "            # Move pipeline to CPU first\n",
        "            try:\n",
        "                self.pipe = self.pipe.to(\"cpu\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Delete references\n",
        "        if hasattr(self, 'pipe'):\n",
        "            del self.pipe\n",
        "        if hasattr(self, 'generator'):\n",
        "            del self.generator\n",
        "\n",
        "        # Reset attributes\n",
        "        self.pipe = None\n",
        "        self.generator = None\n",
        "\n",
        "        # Force garbage collection\n",
        "        gc.collect()\n",
        "\n",
        "        # Clear CUDA cache\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "    def load_model(self, model_id, use_cpu_offload=True, use_sequential_offload=False):\n",
        "        \"\"\"Load model with advanced Colab optimizations\"\"\"\n",
        "\n",
        "        if not model_id.strip():\n",
        "            return \"‚ùå Please enter a model ID\"\n",
        "\n",
        "        if model_id == self.current_model:\n",
        "            return f\"‚ö†Ô∏è Model {model_id} already loaded\"\n",
        "\n",
        "        try:\n",
        "            # Clear existing model\n",
        "            if self.pipe is not None:\n",
        "                self.clear_memory()\n",
        "\n",
        "            status_msg = f\"üîÑ Loading {model_id}...\"\n",
        "            print(status_msg)\n",
        "\n",
        "            # Load with memory optimizations for Colab\n",
        "            load_kwargs = {\n",
        "                \"torch_dtype\": torch.float16,\n",
        "                \"safety_checker\": None,\n",
        "                \"requires_safety_checker\": False,\n",
        "                \"low_cpu_mem_usage\": True,\n",
        "                \"use_safetensors\": True\n",
        "            }\n",
        "\n",
        "            # Try to load model\n",
        "            self.pipe = DiffusionPipeline.from_pretrained(model_id, **load_kwargs)\n",
        "\n",
        "            # Explicitly cast to float16 after loading\n",
        "            if torch.cuda.is_available():\n",
        "                self.pipe = self.pipe.to(torch.float16)\n",
        "\n",
        "            # Apply memory optimizations based on settings\n",
        "            if use_sequential_offload and hasattr(self.pipe, 'enable_sequential_cpu_offload'):\n",
        "                # Most memory efficient but slowest\n",
        "                self.pipe.enable_sequential_cpu_offload()\n",
        "            elif use_cpu_offload and hasattr(self.pipe, 'enable_model_cpu_offload'):\n",
        "                # Good balance of speed and memory\n",
        "                self.pipe.enable_model_cpu_offload()\n",
        "            else:\n",
        "                # Keep everything on GPU if memory allows\n",
        "                self.pipe = self.pipe.to(self.device)\n",
        "\n",
        "\n",
        "            # Enable memory efficient attention\n",
        "            try:\n",
        "                if hasattr(self.pipe, 'enable_xformers_memory_efficient_attention'):\n",
        "                    self.pipe.enable_xformers_memory_efficient_attention()\n",
        "                elif hasattr(self.pipe, 'enable_attention_slicing'):\n",
        "                    self.pipe.enable_attention_slicing()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Enable VAE slicing for large images\n",
        "            if hasattr(self.pipe, 'enable_vae_slicing'):\n",
        "                self.pipe.enable_vae_slicing()\n",
        "\n",
        "            # Enable VAE tiling for very large images\n",
        "            if hasattr(self.pipe, 'enable_vae_tiling'):\n",
        "                self.pipe.enable_vae_tiling()\n",
        "\n",
        "            self.current_model = model_id\n",
        "            memory_info = self.check_memory()\n",
        "\n",
        "            return f\"‚úÖ Successfully loaded: {model_id}\\n{memory_info}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            self.clear_memory()\n",
        "            error_msg = str(e)\n",
        "            if \"out of memory\" in error_msg.lower():\n",
        "                return f\"‚ùå Out of memory loading {model_id}. Try enabling CPU offload or using a smaller model.\"\n",
        "            return f\"‚ùå Error loading {model_id}: {error_msg}\"\n",
        "\n",
        "# Initialize generator\n",
        "generator = OptimizedImageGenerator()\n",
        "\n",
        "def generate_image(\n",
        "    prompt,\n",
        "    negative_prompt=\"blurry, low quality, distorted, deformed\",\n",
        "    steps=25,\n",
        "    guidance=7.5,\n",
        "    width=1024,\n",
        "    height=1024,\n",
        "    seed=None,\n",
        "    use_random_seed=True,\n",
        "    batch_size=1\n",
        "):\n",
        "    if not generator.pipe:\n",
        "        return None, \"‚ö†Ô∏è Please load a model first!\"\n",
        "\n",
        "    if not prompt.strip():\n",
        "        return None, \"‚ö†Ô∏è Please enter a prompt!\"\n",
        "\n",
        "    try:\n",
        "        # Memory check before generation\n",
        "        memory_info = generator.check_memory()\n",
        "        print(f\"Pre-generation: {memory_info}\")\n",
        "\n",
        "        # Handle seed\n",
        "        if use_random_seed or seed is None:\n",
        "            seed = int(time.time() * 1000000) % 2147483647\n",
        "\n",
        "        # Create generator for reproducibility\n",
        "        torch_generator = torch.Generator(device=generator.device).manual_seed(int(seed))\n",
        "\n",
        "        # Clear cache before generation\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Generation parameters\n",
        "        gen_kwargs = {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"num_inference_steps\": int(steps),\n",
        "            \"guidance_scale\": float(guidance),\n",
        "            \"width\": int(width),\n",
        "            \"height\": int(height),\n",
        "            \"generator\": torch_generator,\n",
        "            \"num_images_per_prompt\": int(batch_size)\n",
        "        }\n",
        "\n",
        "        # Generate image\n",
        "        start_time = time.time()\n",
        "        result = generator.pipe(**gen_kwargs)\n",
        "        generation_time = time.time() - start_time\n",
        "\n",
        "        # Get the first image\n",
        "        image = result.images[0] if result.images else None\n",
        "\n",
        "        # Post-generation cleanup\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        success_msg = f\"‚úÖ Generated in {generation_time:.2f}s\\nSeed: {seed}\\n{generator.check_memory()}\"\n",
        "\n",
        "        return image, success_msg\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        if \"out of memory\" in error_msg.lower():\n",
        "            # Aggressive cleanup on OOM\n",
        "            generator.clear_memory()\n",
        "            return None, \"‚ùå Out of memory during generation. Try reducing image size, steps, or batch size.\"\n",
        "        return None, f\"‚ùå Generation failed: {error_msg}\"\n",
        "\n",
        "def clear_all_memory():\n",
        "    \"\"\"Manual memory cleanup function\"\"\"\n",
        "    generator.clear_memory()\n",
        "    return \"üßπ Memory cleared!\"\n",
        "\n",
        "# Custom CSS for better Colab appearance\n",
        "css = \"\"\"\n",
        ".gradio-container {\n",
        "    font-family: 'IBM Plex Sans', sans-serif;\n",
        "}\n",
        ".gr-button {\n",
        "    color: white;\n",
        "    border-color: #9D5CFF;\n",
        "    background: #9D5CFF;\n",
        "}\n",
        ".gr-button:hover {\n",
        "    border-color: #9D5CFF;\n",
        "    background: #9D5CFF;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks(title=\"üöÄ Optimized AI Image Generator - Colab Edition\", css=css, theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üöÄ Optimized AI Image Generator for Google Colab\n",
        "\n",
        "    **Key Optimizations:**\n",
        "    - üß† Smart memory management with automatic cleanup\n",
        "    - üîÑ CPU/GPU offloading options for memory-constrained environments\n",
        "    - ‚ö° Efficient attention mechanisms (xFormers/slicing)\n",
        "    - üéØ VAE optimizations for large image generation\n",
        "    - üìä Real-time memory monitoring\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Model Management Section\n",
        "            gr.Markdown(\"### üéØ Model Management\")\n",
        "\n",
        "            model_url = gr.Textbox(\n",
        "                label=\"Hugging Face Model ID\",\n",
        "                placeholder=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "                value=\"runwayml/stable-diffusion-v1-5\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                use_cpu_offload = gr.Checkbox(\n",
        "                    label=\"Enable CPU Offloading\",\n",
        "                    value=True,\n",
        "                    info=\"Recommended for Colab (saves VRAM)\"\n",
        "                )\n",
        "                use_sequential_offload = gr.Checkbox(\n",
        "                    label=\"Sequential Offloading\",\n",
        "                    value=False,\n",
        "                    info=\"Maximum memory saving (slower)\"\n",
        "                )\n",
        "\n",
        "            load_btn = gr.Button(\"üîÑ Load Model\", variant=\"primary\", size=\"lg\")\n",
        "            clear_btn = gr.Button(\"üßπ Clear Memory\", variant=\"secondary\")\n",
        "\n",
        "            model_status = gr.Markdown(\"**Status:** No model loaded\")\n",
        "\n",
        "            # Generation Parameters\n",
        "            gr.Markdown(\"### ‚öôÔ∏è Generation Settings\")\n",
        "\n",
        "            prompt = gr.Textbox(\n",
        "                label=\"Prompt\",\n",
        "                placeholder=\"A beautiful sunset over mountains, detailed, artistic\",\n",
        "                lines=3\n",
        "            )\n",
        "\n",
        "            negative_prompt = gr.Textbox(\n",
        "                label=\"Negative Prompt\",\n",
        "                value=\"blurry, low quality, distorted, deformed, ugly, bad anatomy\",\n",
        "                lines=2\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                steps = gr.Slider(10, 50, value=25, step=1, label=\"Inference Steps\")\n",
        "                guidance = gr.Slider(1.0, 20.0, value=7.5, step=0.5, label=\"Guidance Scale\")\n",
        "\n",
        "            with gr.Row():\n",
        "                width = gr.Slider(512, 1536, value=1024, step=64, label=\"Width\")\n",
        "                height = gr.Slider(512, 1536, value=1024, step=64, label=\"Height\")\n",
        "\n",
        "            with gr.Row():\n",
        "                seed = gr.Number(label=\"Seed (optional)\", precision=0, value=42)\n",
        "                random_seed = gr.Checkbox(label=\"Random Seed\", value=True)\n",
        "                batch_size = gr.Slider(1, 4, value=1, step=1, label=\"Batch Size\")\n",
        "\n",
        "            generate_btn = gr.Button(\"üé® Generate Image\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            # Output Section\n",
        "            gr.Markdown(\"### üñºÔ∏è Generated Image\")\n",
        "\n",
        "            output_image = gr.Image(\n",
        "                label=\"Generated Image\",\n",
        "                type=\"pil\",\n",
        "                height=600,\n",
        "                show_download_button=True\n",
        "            )\n",
        "\n",
        "            status = gr.Markdown(\"**Ready to generate!**\")\n",
        "\n",
        "            # Memory Monitor\n",
        "            gr.Markdown(\"### üìä Memory Monitor\")\n",
        "            memory_display = gr.Markdown(\"Click 'Check Memory' to see current usage\")\n",
        "            memory_check_btn = gr.Button(\"üìä Check Memory\")\n",
        "\n",
        "    # Tips Section\n",
        "    gr.Markdown(\"\"\"\n",
        "    ### üí° Colab Usage Tips\n",
        "\n",
        "    1. **Memory Management**: Enable CPU offloading if you encounter out-of-memory errors\n",
        "    2. **Performance**: Start with smaller images (768x768) and fewer steps (20-25) for faster generation\n",
        "    3. **Quality**: Use negative prompts to improve image quality\n",
        "    4. **Stability**: Clear memory between model switches to prevent crashes\n",
        "    5. **Colab Limits**: Free tier has session limits - save your generated images!\n",
        "    \"\"\")\n",
        "\n",
        "    # Event handlers\n",
        "    load_btn.click(\n",
        "        fn=generator.load_model,\n",
        "        inputs=[model_url, use_cpu_offload, use_sequential_offload],\n",
        "        outputs=model_status,\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=clear_all_memory,\n",
        "        outputs=model_status\n",
        "    )\n",
        "\n",
        "    generate_btn.click(\n",
        "        fn=generate_image,\n",
        "        inputs=[\n",
        "            prompt, negative_prompt, steps, guidance,\n",
        "            width, height, seed, random_seed, batch_size\n",
        "        ],\n",
        "        outputs=[output_image, status],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    memory_check_btn.click(\n",
        "        fn=generator.check_memory,\n",
        "        outputs=memory_display\n",
        "    )\n",
        "\n",
        "# Launch configuration optimized for Colab\n",
        "if __name__ == \"__main__\":\n",
        "    # For Colab, use share=True to get public URL\n",
        "    demo.launch(\n",
        "        share=True,           # Creates shareable link\n",
        "        debug=True,          # Disable debug for cleaner output\n",
        "        show_error=True,      # Show errors for debugging\n",
        "        server_name=\"0.0.0.0\", # Allow external connections\n",
        "        server_port=7860,     # Standard port\n",
        "        inbrowser=True,       # Auto-open in browser\n",
        "        inline=False          # Don't inline in notebook\n",
        "    )"
      ],
      "metadata": {
        "id": "th6D9yMiruY3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}