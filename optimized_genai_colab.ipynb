{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQJWO7AmnRqlItpsjL/gdI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi-0203/Optimized-AI-Image-Generator-for-Google-Colab/blob/main/optimized_genai_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip install diffusers==0.32.2\n",
        "!pip install transformers==4.49"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IwO6lySmzRlr",
        "outputId": "b2f2fe3e-debd-4cc8-8ac1-315e944f1e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Collecting xformers\n",
            "  Downloading https://download.pytorch.org/whl/cu126/xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers) (2.0.2)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from xformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->xformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->xformers) (3.0.3)\n",
            "Downloading https://download.pytorch.org/whl/cu126/xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers\n",
            "Successfully installed xformers-0.0.32.post2\n",
            "Collecting diffusers==0.32.2\n",
            "  Downloading diffusers-0.32.2-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from diffusers==0.32.2) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers==0.32.2) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.32.2) (0.35.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers==0.32.2) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.32.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers==0.32.2) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.32.2) (0.6.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers==0.32.2) (11.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.2) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.2) (6.0.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.32.2) (1.1.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->diffusers==0.32.2) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.32.2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.32.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.32.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.32.2) (2025.10.5)\n",
            "Downloading diffusers-0.32.2-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diffusers\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.35.1\n",
            "    Uninstalling diffusers-0.35.1:\n",
            "      Successfully uninstalled diffusers-0.35.1\n",
            "Successfully installed diffusers-0.32.2\n",
            "Collecting transformers==4.49\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.49) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.49) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.49)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49) (2025.10.5)\n",
            "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "Successfully installed tokenizers-0.21.4 transformers-4.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from diffusers import DiffusionPipeline, AutoencoderKL\n",
        "import torch\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "from PIL import Image\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output in Colab\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "class OptimizedImageGenerator:\n",
        "    def __init__(self):\n",
        "        self.pipe = None\n",
        "        self.current_model = None\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.generator = None\n",
        "\n",
        "        # Setup for Colab optimization\n",
        "        if torch.cuda.is_available():\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    def check_memory(self):\n",
        "        \"\"\"Check and display current GPU memory usage\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
        "            reserved = torch.cuda.memory_reserved() / 1024**3    # GB\n",
        "            return f\"🔍 GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\"\n",
        "        return \"CPU mode - No GPU memory tracking\"\n",
        "\n",
        "    def clear_memory(self):\n",
        "        \"\"\"Aggressive memory cleanup for Colab\"\"\"\n",
        "        if hasattr(self, 'pipe') and self.pipe is not None:\n",
        "            # Move pipeline to CPU first\n",
        "            try:\n",
        "                self.pipe = self.pipe.to(\"cpu\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Delete references\n",
        "        if hasattr(self, 'pipe'):\n",
        "            del self.pipe\n",
        "        if hasattr(self, 'generator'):\n",
        "            del self.generator\n",
        "\n",
        "        # Reset attributes\n",
        "        self.pipe = None\n",
        "        self.generator = None\n",
        "\n",
        "        # Force garbage collection\n",
        "        gc.collect()\n",
        "\n",
        "        # Clear CUDA cache\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "    def load_model(self, model_id, use_cpu_offload=True, use_sequential_offload=False):\n",
        "        \"\"\"Load model with advanced Colab optimizations\"\"\"\n",
        "\n",
        "        if not model_id.strip():\n",
        "            return \"❌ Please enter a model ID\"\n",
        "\n",
        "        if model_id == self.current_model:\n",
        "            return f\"⚠️ Model {model_id} already loaded\"\n",
        "\n",
        "        try:\n",
        "            # Clear existing model\n",
        "            if self.pipe is not None:\n",
        "                self.clear_memory()\n",
        "\n",
        "            status_msg = f\"🔄 Loading {model_id}...\"\n",
        "            print(status_msg)\n",
        "\n",
        "            # Load with memory optimizations for Colab\n",
        "            load_kwargs = {\n",
        "                \"torch_dtype\": torch.float16,\n",
        "                \"safety_checker\": None,\n",
        "                \"requires_safety_checker\": False,\n",
        "                \"low_cpu_mem_usage\": True,\n",
        "                \"use_safetensors\": True\n",
        "            }\n",
        "\n",
        "            # Try to load model\n",
        "            self.pipe = DiffusionPipeline.from_pretrained(model_id, **load_kwargs)\n",
        "\n",
        "            # Explicitly cast to float16 after loading\n",
        "            if torch.cuda.is_available():\n",
        "                self.pipe = self.pipe.to(torch.float16)\n",
        "\n",
        "            # Apply memory optimizations based on settings\n",
        "            if use_sequential_offload and hasattr(self.pipe, 'enable_sequential_cpu_offload'):\n",
        "                # Most memory efficient but slowest\n",
        "                self.pipe.enable_sequential_cpu_offload()\n",
        "            elif use_cpu_offload and hasattr(self.pipe, 'enable_model_cpu_offload'):\n",
        "                # Good balance of speed and memory\n",
        "                self.pipe.enable_model_cpu_offload()\n",
        "            else:\n",
        "                # Keep everything on GPU if memory allows\n",
        "                self.pipe = self.pipe.to(self.device)\n",
        "\n",
        "\n",
        "            # Enable memory efficient attention\n",
        "            try:\n",
        "                if hasattr(self.pipe, 'enable_xformers_memory_efficient_attention'):\n",
        "                    self.pipe.enable_xformers_memory_efficient_attention()\n",
        "                elif hasattr(self.pipe, 'enable_attention_slicing'):\n",
        "                    self.pipe.enable_attention_slicing()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Enable VAE slicing for large images\n",
        "            if hasattr(self.pipe, 'enable_vae_slicing'):\n",
        "                self.pipe.enable_vae_slicing()\n",
        "\n",
        "            # Enable VAE tiling for very large images\n",
        "            if hasattr(self.pipe, 'enable_vae_tiling'):\n",
        "                self.pipe.enable_vae_tiling()\n",
        "\n",
        "            self.current_model = model_id\n",
        "            memory_info = self.check_memory()\n",
        "\n",
        "            return f\"✅ Successfully loaded: {model_id}\\n{memory_info}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            self.clear_memory()\n",
        "            error_msg = str(e)\n",
        "            if \"out of memory\" in error_msg.lower():\n",
        "                return f\"❌ Out of memory loading {model_id}. Try enabling CPU offload or using a smaller model.\"\n",
        "            return f\"❌ Error loading {model_id}: {error_msg}\"\n",
        "\n",
        "# Initialize generator\n",
        "generator = OptimizedImageGenerator()\n",
        "\n",
        "def generate_image(\n",
        "    prompt,\n",
        "    negative_prompt=\"blurry, low quality, distorted, deformed\",\n",
        "    steps=25,\n",
        "    guidance=7.5,\n",
        "    width=1024,\n",
        "    height=1024,\n",
        "    seed=None,\n",
        "    use_random_seed=True,\n",
        "    batch_size=1\n",
        "):\n",
        "    if not generator.pipe:\n",
        "        return None, \"⚠️ Please load a model first!\"\n",
        "\n",
        "    if not prompt.strip():\n",
        "        return None, \"⚠️ Please enter a prompt!\"\n",
        "\n",
        "    try:\n",
        "        # Memory check before generation\n",
        "        memory_info = generator.check_memory()\n",
        "        print(f\"Pre-generation: {memory_info}\")\n",
        "\n",
        "        # Handle seed\n",
        "        if use_random_seed or seed is None:\n",
        "            seed = int(time.time() * 1000000) % 2147483647\n",
        "\n",
        "        # Create generator for reproducibility\n",
        "        torch_generator = torch.Generator(device=generator.device).manual_seed(int(seed))\n",
        "\n",
        "        # Clear cache before generation\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Generation parameters\n",
        "        gen_kwargs = {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"num_inference_steps\": int(steps),\n",
        "            \"guidance_scale\": float(guidance),\n",
        "            \"width\": int(width),\n",
        "            \"height\": int(height),\n",
        "            \"generator\": torch_generator,\n",
        "            \"num_images_per_prompt\": int(batch_size)\n",
        "        }\n",
        "\n",
        "        # Generate image\n",
        "        start_time = time.time()\n",
        "        result = generator.pipe(**gen_kwargs)\n",
        "        generation_time = time.time() - start_time\n",
        "\n",
        "        # Get the first image\n",
        "        image = result.images[0] if result.images else None\n",
        "\n",
        "        # Post-generation cleanup\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        success_msg = f\"✅ Generated in {generation_time:.2f}s\\nSeed: {seed}\\n{generator.check_memory()}\"\n",
        "\n",
        "        return image, success_msg\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        if \"out of memory\" in error_msg.lower():\n",
        "            # Aggressive cleanup on OOM\n",
        "            generator.clear_memory()\n",
        "            return None, \"❌ Out of memory during generation. Try reducing image size, steps, or batch size.\"\n",
        "        return None, f\"❌ Generation failed: {error_msg}\"\n",
        "\n",
        "def clear_all_memory():\n",
        "    \"\"\"Manual memory cleanup function\"\"\"\n",
        "    generator.clear_memory()\n",
        "    return \"🧹 Memory cleared!\"\n",
        "\n",
        "# Custom CSS for better Colab appearance\n",
        "css = \"\"\"\n",
        ".gradio-container {\n",
        "    font-family: 'IBM Plex Sans', sans-serif;\n",
        "}\n",
        ".gr-button {\n",
        "    color: white;\n",
        "    border-color: #9D5CFF;\n",
        "    background: #9D5CFF;\n",
        "}\n",
        ".gr-button:hover {\n",
        "    border-color: #9D5CFF;\n",
        "    background: #9D5CFF;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks(title=\"🚀 Optimized AI Image Generator - Colab Edition\", css=css, theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # 🚀 Optimized AI Image Generator for Google Colab\n",
        "\n",
        "    **Key Optimizations:**\n",
        "    - 🧠 Smart memory management with automatic cleanup\n",
        "    - 🔄 CPU/GPU offloading options for memory-constrained environments\n",
        "    - ⚡ Efficient attention mechanisms (xFormers/slicing)\n",
        "    - 🎯 VAE optimizations for large image generation\n",
        "    - 📊 Real-time memory monitoring\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Model Management Section\n",
        "            gr.Markdown(\"### 🎯 Model Management\")\n",
        "\n",
        "            model_url = gr.Textbox(\n",
        "                label=\"Hugging Face Model ID\",\n",
        "                placeholder=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "                value=\"runwayml/stable-diffusion-v1-5\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                use_cpu_offload = gr.Checkbox(\n",
        "                    label=\"Enable CPU Offloading\",\n",
        "                    value=True,\n",
        "                    info=\"Recommended for Colab (saves VRAM)\"\n",
        "                )\n",
        "                use_sequential_offload = gr.Checkbox(\n",
        "                    label=\"Sequential Offloading\",\n",
        "                    value=False,\n",
        "                    info=\"Maximum memory saving (slower)\"\n",
        "                )\n",
        "\n",
        "            load_btn = gr.Button(\"🔄 Load Model\", variant=\"primary\", size=\"lg\")\n",
        "            clear_btn = gr.Button(\"🧹 Clear Memory\", variant=\"secondary\")\n",
        "\n",
        "            model_status = gr.Markdown(\"**Status:** No model loaded\")\n",
        "\n",
        "            # Generation Parameters\n",
        "            gr.Markdown(\"### ⚙️ Generation Settings\")\n",
        "\n",
        "            prompt = gr.Textbox(\n",
        "                label=\"Prompt\",\n",
        "                placeholder=\"A beautiful sunset over mountains, detailed, artistic\",\n",
        "                lines=3\n",
        "            )\n",
        "\n",
        "            negative_prompt = gr.Textbox(\n",
        "                label=\"Negative Prompt\",\n",
        "                value=\"blurry, low quality, distorted, deformed, ugly, bad anatomy\",\n",
        "                lines=2\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                steps = gr.Slider(10, 50, value=25, step=1, label=\"Inference Steps\")\n",
        "                guidance = gr.Slider(1.0, 20.0, value=7.5, step=0.5, label=\"Guidance Scale\")\n",
        "\n",
        "            with gr.Row():\n",
        "                width = gr.Slider(512, 1536, value=1024, step=64, label=\"Width\")\n",
        "                height = gr.Slider(512, 1536, value=1024, step=64, label=\"Height\")\n",
        "\n",
        "            with gr.Row():\n",
        "                seed = gr.Number(label=\"Seed (optional)\", precision=0, value=42)\n",
        "                random_seed = gr.Checkbox(label=\"Random Seed\", value=True)\n",
        "                batch_size = gr.Slider(1, 4, value=1, step=1, label=\"Batch Size\")\n",
        "\n",
        "            generate_btn = gr.Button(\"🎨 Generate Image\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            # Output Section\n",
        "            gr.Markdown(\"### 🖼️ Generated Image\")\n",
        "\n",
        "            output_image = gr.Image(\n",
        "                label=\"Generated Image\",\n",
        "                type=\"pil\",\n",
        "                height=600,\n",
        "                show_download_button=True\n",
        "            )\n",
        "\n",
        "            status = gr.Markdown(\"**Ready to generate!**\")\n",
        "\n",
        "            # Memory Monitor\n",
        "            gr.Markdown(\"### 📊 Memory Monitor\")\n",
        "            memory_display = gr.Markdown(\"Click 'Check Memory' to see current usage\")\n",
        "            memory_check_btn = gr.Button(\"📊 Check Memory\")\n",
        "\n",
        "    # Tips Section\n",
        "    gr.Markdown(\"\"\"\n",
        "    ### 💡 Colab Usage Tips\n",
        "\n",
        "    1. **Memory Management**: Enable CPU offloading if you encounter out-of-memory errors\n",
        "    2. **Performance**: Start with smaller images (768x768) and fewer steps (20-25) for faster generation\n",
        "    3. **Quality**: Use negative prompts to improve image quality\n",
        "    4. **Stability**: Clear memory between model switches to prevent crashes\n",
        "    5. **Colab Limits**: Free tier has session limits - save your generated images!\n",
        "    \"\"\")\n",
        "\n",
        "    # Event handlers\n",
        "    load_btn.click(\n",
        "        fn=generator.load_model,\n",
        "        inputs=[model_url, use_cpu_offload, use_sequential_offload],\n",
        "        outputs=model_status,\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=clear_all_memory,\n",
        "        outputs=model_status\n",
        "    )\n",
        "\n",
        "    generate_btn.click(\n",
        "        fn=generate_image,\n",
        "        inputs=[\n",
        "            prompt, negative_prompt, steps, guidance,\n",
        "            width, height, seed, random_seed, batch_size\n",
        "        ],\n",
        "        outputs=[output_image, status],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "    memory_check_btn.click(\n",
        "        fn=generator.check_memory,\n",
        "        outputs=memory_display\n",
        "    )\n",
        "\n",
        "# Launch configuration optimized for Colab\n",
        "if __name__ == \"__main__\":\n",
        "    # For Colab, use share=True to get public URL\n",
        "    demo.launch(\n",
        "        share=True,           # Creates shareable link\n",
        "        debug=True,          # Disable debug for cleaner output\n",
        "        show_error=True,      # Show errors for debugging\n",
        "        server_name=\"0.0.0.0\", # Allow external connections\n",
        "        server_port=7860,     # Standard port\n",
        "        inbrowser=True,       # Auto-open in browser\n",
        "        inline=False          # Don't inline in notebook\n",
        "    )"
      ],
      "metadata": {
        "id": "th6D9yMiruY3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}